Stardog Netty+Protocol Buffers based Connection Protocol, i.e. SNARL on the wire, code named BARK (BigPacket Access for Remote Connections)

Big Packets
-----------

Client & Server are implemented via Netty w/ the payloads being exchanged between them encoded via
Google's Protocol Buffers in an RPC-esque manner.

Netty's built in Protobuf support allows only a single kind of Message to be exchanged over a Channel;
you have to provide the archetype of the Message to be exchanged when you start things up.  Since we
want to exchange more than one type of message between the client and the server, we're using
Protobuf extensions to create the Big Packets API.

The single message type exchanged between client & server is CommandBase, aka the big packet.
CommandBase serves as most of the header information exchanged; there is a handler prior to
the Protobuf layer which enforces frame sizes.  3 bytes are reserved to encode the ensueing frame
size (in bytes) allowing for frames up to 16.7M bytes.  This limits the size of protobuf messages to up
to 16M.

In the Protobuf docs, they don't recommend having Messages larger than 1M.  No explicit reasoning
is provided for this, they just suggest smaller messages or a different system.  Unfortunately
a 2 byte size header only gives us frames of 64k, which is rather small and well under the suggested
threshold.  Some care is taken to limit message size on the server by providing a max number of things
we'll send in a single message, so while we've got nearly 17M of space to use in the message payload,
we're hopefully using only a fraction of that.

I did experiment some to see if there was a performance advantage one way or the other, but there
did not appear to be any appreciable change from 64k to 16.7M frame sizes.  Some things were a hair
faster, some were a bit slower.

CommandBase has 10 reserved spaces in its header to encode the information about the payload.  The ID
of the payload and the type of payload are required.  ID is used to keep requests & responses in sync
and the type is used on both sides to decode the information in the payload of the command header.

CommandBase also reserves information to encode an error code & message, when error code is non-null,
an error occurred on the remote end of the call and the message contains whatever relevant information
is provided w.r.t to the cause of the error.  Payload on errors is empty or otherwise can be ignored,
but we could explore encoding stack trace information if need be in the payload.

Finally, the command has a flag on whether or not it is the end of the command sequence.  This flag is
true by default assuming the entire request or response can be encoded in a single command.  But when
the flag is false, it means there are more commands coming in the future (with the same UID) which contain
additional information about the request or response.  See the section on streaming for more information.

Possible changes
----------------

Initial tests are showing at small scales, we're significantly outperforming the HTTP & Avro protocols,
and at larger scales, even w/ HTTP and still crushing Avro.  I think this is expected at large scale,
10M bytes is 10M bytes no matter if its via HTTP or Netty+Protobuf, that will transfer over the network
as fast as the network goes independent of how we're sending it.  So I think we'll lose our advantage
at large data scale due to this fact, but its good that we're not incurring cost and ending up slower than
plain old HTTP.  So I think to gain speed (at all scales) it just comes down to sending less data.

Re-use
------

The only real hurdle at this point from re-using the protocol stuff again in another project is that the
error codes are represented as an enum and the PacketUtils only knows about that set of errors.  If you change
the error code to an integer and register error transformers like packet transformers, you could drop that
limitation pretty easily.

There's no good way to make sure that packet types don't overlap other than we control all the code so we
make sure they don't.  Probably would want to have something that would over the longer term allow you
to register your types and it would resolve putting them in an un-used block, or throw an error if you
are overlapping with a known registered block.

Other than that, it will be pretty straightforward to re-use the ClientHandler & ServerHandler as a template
for new protocols.  To be fully generic, these would have to avoid using the built in protocol objects, but
that would be pretty painless.

Streaming BigPackets
--------------------

For large requests or responses which are too big to realistically fit into a single Protobuf message, both
due to the size restrictions suggested by the Protobuf authors and due to memory constraints on the client
and/or server.

Thus a request or response can be sent as a stream of BigPackets each with the same UID. The entity receiving
the packets is responsible for collating them into a single coherent datastructure that it is able to work with.

The end of the stream of BigPacket is specified by the header flag endOfSequence being set to true.

BigPacket Diagram
-----------------

+--------------------------------+
|           BigPacket            |
|--------------------------------|
| 1: UID            2: Type      |
| 3: Message        4: ErrorCode |
| 5: EndOfSequence               |
|                                |
|+------------------------------+|
|  Protobuf serialized payload   |
|                                |
|                                |
+--------------------------------+

SNARL Interface
---------------

This is the pseudo-code for the interface implemented by the stardog server for the SNARL protocol.  More details
are discussed for each method in the subsequent sections.


Version				version(Version);
ContextResponse		connect(AuthRequest);

AskResponse			query(QueryRequest);
SelectResponse		query(QueryRequest);
(Compressed)Graph	query(QueryRequest);

ContextResponse		begin(ConnectionContext);
ContextResponse		commit(ConnectionContext);
ContextResponse		rollback(ConnectionContext);

Long				size(ConnectionContext);

Errors
------

Errors are provided back to the client in the header of the packet.  The payload is always empty, and when the
packet type is an error, the error code and message fields are populated with information describing the error
condition and the error message allowing the appropriate exception to be re-created and thrown on the client.

Auth
----
On connect, the client provides its credentials with an authentication request (AuthRequest).  The server will
verify the credentials specified in the request and if successful it will send back a ConnectionContext as
the payload of a ContextResponse.  The resulting ConnectionContext must be provided with all subsequent operations
against the server or the action will not succeed due to the missing context.

SSL is supported on both ends of the connection.  When a client or server is initialized with the SSL, the standard
Netty SSL handler is inserted into the pipeline.  The client requires a valid truststore which contains all of the
certificates of the servers it wishes to connect to.  Conversely, the server must contain a valid keystore containing
its key, which is what the client held server certificates are generated from.  This way, clients are able to
verify that they are talking to a secured server.

In an environment where further security is desired, a trust store could be installed on the server and a keystore
on the client so that they certificate exchange is symmetric.  This lets servers verify that they're talking to
secure clients; however since then the server needs to know ahead of time all clients which will connect to it,
it's probably not ideal for many use cases.

Query
-----

Query requests are initialized with a QueryRequest. There is a single logical method on the server that is called
for all query types.  It makes the determination of which query type to run based on the query in the request
and then returns the appropriate result(s).

Boolean queries are a single response from the server, AskResponse.

Tuple & Graph queries are sent back as a stream of results.  Both Tuple & Graph query responses can provide
compressed results depending on the configuration of the server.  The initial response to a Tuple query is
SelectResponse which lists the bindings (ie the projection) of the executed query.  The rest of the results
in the stream will be either of type TupleResponse for un-compressed results or DictionaryResponse &
CompressedTupleResponse interleaved for compressed results.

In compressed & uncompressed tuple results the bindings are provided back in a list whose elements are in the
*exact* same order as the projection specified in the SelectResponse.  So to properly re-construct the BindingSets
from these result payloads, you'll need the list of binding names provided in the original response.

The results to a graph query immediately come back as a series of GraphResponse's or as DictionaryResponse &
CompressedGraphResponse interleaved.

Compressed RDF Model
--------------------

The compressed RDF model is based on a lookup being intermixed with the normal Statement payloads.  This lookup
is a mapping from int's to strings where the strings are the namespace URI's of all the URI's returned in the
subsequent RDF payload.  So the flow of packets in the stream alternates between dictionary and compressed RDF
statements.  The dictionary is only for the subsequent RDF statements, it does not have to be kept cumulatively
throughout the stream.

We are not currently using the compressed RDF to exchange either RDF or query results; initial tests have shown
that despite sending less data, the overhead of doing the compression makes it a performance loss.  One thing
to try in the future is to make the dictionary relative to the entire stream instead of just the next RDF (or
query result) payload.  This way dictionary transmission will be less frequent, and smaller, at the cost of
greater memory pressure on both sides in keeping the entire dictionary in memory.

Another thing to look at is using the MemoryMappingDictionary on the receiving end -- don't know about this
on the sending end, might be too hard to figure out the diff of new entries to send each time through -- and
then sending just the long id's rather than using the compression to just send namespace id's.


Data Upload
-----------

Uploads use the same packet streaming model as query results returned from the server, except for the fact that the
streams originate on the client rater than the server.  Reading and writing of the streams, while handled by the
BigPacket encoder & decoder (see Overall design) is abstracted by the Sesame Rio implementations of RDF readers
and writers.

Transactions
---------

Search
---------

Reasoning
---------

Overall Design
--------------

Client & server are connected by a Netty pipeline with the client & server handler's at each end.  These handlers are
responsible for dispatching & handling the actual the RPC calls.  Because of Netty's generally asynchronous nature,
the client tracks the outstanding RPC calls with a map of the UUID of the requests to the registered callbacks for each
request.  Both sides of the exchange use LocalPackets to communicate, which are basically a POJO equivalent of the
BigPackets generated by protobuf, the primary difference being that the payloads are the actual Java objects rather than
the serialized bytes of the protobuf objects.  If the client and server are connected by a socket, encoders and decoders
sit in the Channel between them which are responsible for taking LocalPacket objects from the client and serializing them
into BigPackets, possibly as a stream, which can be sent over the wire.  Then there is an equivalent decoder on the other
end which takes the BigPacket(s) and turns them back into a LocalPacket for use on the other side of the request.

It is the responsibility of the BigPacketDecoder and Encoder to handle streaming results from one end of the connection
to the other.  The encoder knows how to write a large query result or RDF as a packet stream and the decoder on the
other end knows how to wait for all the responses in the stream and put them back together as the original object.  This
frees both client & server from having to care about whether or not responses are streamed to the other, that is all
handled by the encoders and decoders when present in the pipeline.

Extending the Protocol
----------------------

Limitations
-----------

Query strings must fit within the single frame they are allocated, so you cannot issue a query larger than ~64k.  Probably
closer to 63k in reality once the overhead of the header for the BigPacket stuff is counted.